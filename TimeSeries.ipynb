{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ...  and so it begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context Version: 1.6.0\n",
      "Java version: 1.8.0\n",
      "Scala Release version: Some(2.10.5)\n",
      "_____________________\n",
      "Spark Context Config:\n",
      "(spark.eventLog.dir,/gpfs/fs01/user/s70e-5f4bdff17b6ca7-347c2e5e0455/events)\n",
      "(spark.app.id,app-20160928013820-0414-7b60fa9f-6ff6-4085-b539-05e273520489)\n",
      "(spark.deploy.resourceScheduler.factory,org.apache.spark.deploy.master.EGOResourceSchedulerFactory)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.ui.retainedJobs,0)\n",
      "(spark.shuffle.service.enabled,true)\n",
      "(spark.driver.port,40721)\n",
      "(spark.externalBlockStore.folderName,spark-bcc1765c-502c-400d-964e-264869423273)\n",
      "(spark.executor.extraJavaOptions,-Djava.security.egd=file:/dev/./urandom)\n",
      "(spark.executor.id,driver)\n",
      "(spark.port.maxRetries,512)\n",
      "(spark.sql.tungsten.enabled,false)\n",
      "(spark.logConf,true)\n",
      "(spark.driver.host,10.143.133.233)\n",
      "(spark.history.fs.logDirectory,/gpfs/fs01/user/s70e-5f4bdff17b6ca7-347c2e5e0455/events)\n",
      "(spark.master,spark://yp-spark-dal09-env5-0027:7082)\n",
      "(spark.r.command,/usr/local/src/bluemix_jupyter_bundle.v19/R/bin/Rscript)\n",
      "(spark.repl.class.uri,http://10.143.133.233:37129)\n",
      "(spark.app.name,IBM Spark Kernel)\n",
      "(spark.ui.enabled,false)\n",
      "(spark.task.maxFailures,10)\n",
      "(spark.sql.unsafe.enabled,false)\n",
      "(spark.ui.retainedStages,0)\n",
      "(spark.worker.ui.retainedExecutors,0)\n",
      "(spark.shuffle.service.port,7340)\n",
      "(spark.master.rest.port,6070)\n",
      "_____________________\n",
      "Spark SQL Context Config:\n"
     ]
    }
   ],
   "source": [
    "/** Reference && Inspiration \n",
    " *\n",
    " *  http://stackoverflow.com/questions/33725500/load-data-from-bluemix-object-store-in-spark\n",
    " *  https://developer.ibm.com/clouddataservices/start-developing-with-spark-and-notebooks/\n",
    " *  https://github.com/ibm-et/spark-kernel/wiki/List-of-Current-Magics-for-the-Spark-Kernel\n",
    " *\n",
    " */\n",
    "println(\"Spark Context Version: \" + sc.version);\n",
    "println(\"Java version: \" + scala.util.Properties.javaVersion);\n",
    "println(\"Scala Release version: \" + scala.util.Properties.releaseVersion);\n",
    "println(\"_____________________\");\n",
    "println(\"Spark Context Config:\");\n",
    "sc.getConf.getAll.foreach(println);\n",
    "println(\"_____________________\");\n",
    "println(\"Spark SQL Context Config:\");\n",
    "sqlctx.getAllConfs.foreach(println);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from https://github.com/joshisa/lightning-scala/raw/master/dist/lightning-scala_2.10-0.2.0.jar\n",
      "Finished download of lightning-scala_2.10-0.2.0.jar\n"
     ]
    }
   ],
   "source": [
    "/** \n",
    " *  Load a custom Lightning Scala Client for Visualizations \n",
    " */\n",
    "%AddJar https://github.com/joshisa/lightning-scala/raw/master/dist/lightning-scala_2.10-0.2.0.jar -f\n",
    "val b = \"SWYgc2hlIHdlaWdocyB0aGUgc2FtZSBhcyBhIGR1Y2ssIHNoZSdzIG1hZGUgb2Ygd29vZCBhbmQgdGhlcmVmb3JlIGEgd2l0Y2guIH5Nb250eSBQeXRob24=\"\n",
    "val bo = \"TGV0J3MgaG9wZSB0aGF0IG91ciBkYXRhIGFuYWx5c2lzIGlzIGEgd2VlIGJpdCBiZXR0ZXIgdGhhbiB0aGUgTW9udHkgUHl0aG9uIG1lZGlldmFsIGxvZ2ljIG9mIGFib3Zl\"\n",
    "val boo = \"RG8gd2hhdCBJIGRvLiBIb2xkIHRpZ2h0IGFuZCBwcmV0ZW5kIGl04oCZcyBhIHBsYW4hIH5Eci4gV2hv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If she weighs the same as a duck, she's made of wood and therefore a witch. ~Monty Python\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " *  Standard and custom imports\n",
    " *\n",
    " *  Imports help with data formats, time calculations and visualizations\n",
    " *\n",
    " */\n",
    "\n",
    "import sys.process._\n",
    "import scala.util.control.NonFatal\n",
    "import java.util.Base64\n",
    "import java.nio.charset.StandardCharsets\n",
    "import play.api.libs.json.Json\n",
    "import org.apache.spark.sql.functions.sum\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DoubleType, Metadata};\n",
    "\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "import System.nanoTime\n",
    "import java.util.concurrent.TimeUnit\n",
    "import scala.util.Random\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import java.text.DecimalFormat\n",
    "\n",
    "import org.viz.lightning._\n",
    "\n",
    "/**\n",
    " *  Dumb Code to obscure strings\n",
    " */\n",
    " \n",
    "def createMagic(mystery: String): String = {\n",
    "  Base64.getEncoder.encodeToString(mystery.getBytes(StandardCharsets.UTF_8))\n",
    "}\n",
    "\n",
    "/**\n",
    " *  Equally dumb code to make obscure strings less obscure\n",
    " */\n",
    " \n",
    "def unfurl(mystery: String): String = {\n",
    "  return new String(Base64.getDecoder().decode(mystery), StandardCharsets.UTF_8)\n",
    "}\n",
    "\n",
    "/**\n",
    " *  Important tuning optimization for potential Join operations\n",
    " */\n",
    "\n",
    "val sqlctx = new SQLContext(sc)\n",
    "sqlctx.setConf(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "val profileFormat = new DecimalFormat(\"#.###########\")\n",
    "\n",
    "println(unfurl(b))\n",
    "println(unfurl(bo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stand up Scaffolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do what I do. Hold tight and pretend it’s a plan! ~Dr. Who\n"
     ]
    }
   ],
   "source": [
    "/**\n",
    " *  Lightning provides API-based access to reproducible web visualizations.\n",
    " *  http://lightning-viz.org/\n",
    " *  @param host should not contain a trailing slash\n",
    " */ \n",
    "val lgn = Lightning(host=\"https://lightningviz.mybluemix.net\");\n",
    "lgn.auth = None\n",
    "lgn.createSession()\n",
    "println(unfurl(boo))\n",
    "println(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**\n",
    " *  Useful for formatted console output of test results and calculated values\n",
    " */\n",
    "\n",
    "def printAnalysis(expected: Array[Double], calculated: Array[Double]):Any = {\n",
    "    val sb = new StringBuilder\n",
    "    (expected,calculated).zipped.foreach {(expected,result) =>\n",
    "        sb append \"Expected: \" + expected + \" Calculated: \" + result\n",
    "        if (expected == result) sb append \"   PASS\" else sb append \"   FAIL :-(\"                                                          \n",
    "        println(sb)\n",
    "        sb.setLength(0)\n",
    "    }\n",
    "}\n",
    "\n",
    "def createTestDF(testValues:Array[Double]): org.apache.spark.sql.DataFrame = {\n",
    "    val schema = StructType(\n",
    "            Seq(\n",
    "              StructField(\"TestValues\", DoubleType, false, Metadata.fromJson(\"\"\"{\"description\":\"Series of Test Input Values\"}\"\"\"))\n",
    "              )\n",
    "            )\n",
    "    val testinput:org.apache.spark.rdd.RDD[Double] = sc.parallelize(testValues)\n",
    "    val rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = testinput map {\n",
    "          case testvalue => org.apache.spark.sql.Row(testvalue)\n",
    "        }\n",
    "\n",
    "    return sqlctx.createDataFrame(rows,schema)\n",
    "}\n",
    "\n",
    "def metaPrint(dataframe: org.apache.spark.sql.DataFrame) = {\n",
    "    println(\"Meta Description\")\n",
    "    println(\"================\")\n",
    "    dataframe.schema.foreach{s => println(s\"Column: ${s.name} [${(Json.parse(s.metadata.toString)\\ \"description\").as[String]}]\")}\n",
    "    println(\".\")\n",
    "    dataframe.printSchema()\n",
    "}\n",
    "\n",
    "def display (values: Array[Array[Double]],\n",
    "            lgn: org.viz.lightning.Lightning = Lightning(host=\"https://lightningviz.mybluemix.net\"), \n",
    "            xaxis: String = \"x-Axis\",\n",
    "            yaxis: String = \"y-Axis\"): Either[com.ibm.spark.magic.CellMagicOutput,com.ibm.spark.magic.LineMagicOutput] = {\n",
    "                                          \n",
    "    val viz = lgn.line(values, xaxis=xaxis, yaxis=yaxis)\n",
    "    val viz2html = viz.getHTML\n",
    "    return kernel.magics.html(s\"\"\"${viz2html}\"\"\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/**\n",
    " *  Useful for smallish datasets to calculate:\n",
    " *  Simple, Central and Exponential Moving Averages as well as 1st and 2nd Differentials\n",
    " *  For larger datasets, consider the Cloudera ts lib\n",
    " *  @constructor create a new notebookTimeSeries object taking an Array[Double]\n",
    " *  @param dataframe: Spark SQL Dataframe containing 1...N columns\n",
    " *  @param colnames: Sequence of column names to be aggregated (summed) and\n",
    " *                   treated as a single Array for visualization.\n",
    " *  @author Sanjay Joshi (joshisa@us.ibm.com)\n",
    " *  @version 1.0\n",
    " *  @todo Add more functionality.\n",
    " *\n",
    " *  Copyright 2016 IBM\n",
    " */\n",
    "\n",
    "class notebookTimeSeries(dataframe: org.apache.spark.sql.DataFrame, colnames: Seq[String]) {\n",
    "  import org.viz.lightning._\n",
    "  import java.math.MathContext\n",
    "    \n",
    "  val values: Array[Double] = dataframe.select(colnames.head, colnames.tail: _*).rdd.map{\n",
    "      case org.apache.spark.sql.Row(row:Int) => {\n",
    "          var sumInt = 0.0\n",
    "          for (i <- 0 until row.length) {\n",
    "              sumInt += row(i).toDouble\n",
    "          }\n",
    "          sumInt\n",
    "      }\n",
    "      case org.apache.spark.sql.Row(row:Double) => {\n",
    "          var sumDub = 0.0\n",
    "          for (i <- 0 until row.length) {\n",
    "              sumDub += row(i)\n",
    "          }\n",
    "          sumDub\n",
    "      }\n",
    "  }.collect()\n",
    "    \n",
    "  println(\"Nanite data probes launched ... \")\n",
    "    \n",
    "  private def roundME(bignumber: Double, precision: Int = 4):Double = {\n",
    "    var bd:BigDecimal = BigDecimal(bignumber)\n",
    "    bd = bd.round(new MathContext(precision))\n",
    "    return bd.doubleValue()\n",
    "  }  \n",
    "    \n",
    "  def display (lgn: org.viz.lightning.Lightning = Lightning(host=\"https://lightningviz.mybluemix.net\"), \n",
    "               xaxis: String = \"x-Axis\",\n",
    "               yaxis: String = \"y-Axis\"): Either[com.ibm.spark.magic.CellMagicOutput,com.ibm.spark.magic.LineMagicOutput] = {\n",
    "                                          \n",
    "    val viz = lgn.line(Array(values), xaxis=xaxis, yaxis=yaxis)\n",
    "    val viz2html = viz.getHTML\n",
    "    return kernel.magics.html(s\"\"\"${viz2html}\"\"\");\n",
    "  }\n",
    "    \n",
    "  def firstdiffy(): Array[Double] = {\n",
    "      return ((values drop 1, values).zipped.map(_-_) map math.abs)\n",
    "  }\n",
    "    \n",
    "  def secdiffy(): Array[Double]  = {\n",
    "      return ((firstdiffy() drop 1, firstdiffy()).zipped.map(_-_) map math.abs)\n",
    "  }\n",
    "  \n",
    "  def simpleMA(period: Int, step: Int = 1): Array[Double] = {\n",
    "      (List.make(period - 1, 0.0) ++ (values sliding (period,step) map (_ sum) map (_ / period) map (roundME(_)))).toArray\n",
    "  }\n",
    "    \n",
    "  private def cmaCalc(period: Int, step: Int = 1): Array[Double] = {\n",
    "      val buffy = (List.make(period - 1, 0.0) ++ (values sliding (period,step) map (_ sum) map (_ / period) map (roundME(_)))).toBuffer\n",
    "      buffy.remove(0, ((period - 1) / 2))\n",
    "      return buffy.toArray\n",
    "  }\n",
    "\n",
    "  def centralMA(period: Int): Array[Double] = {\n",
    "      try {\n",
    "          period match {\n",
    "              case bad if bad < 2 => {\n",
    "                  println(s\"WARNING: window size of ${bad} provided. Central Moving Average cannot be computed\")\n",
    "                  val window = 0\n",
    "                  return Array(0.0)\n",
    "              }\n",
    "              case even if even % 2 == 0 => {\n",
    "                  println(s\"WARNING: Even window size of $period detected. Incrementing window size from ${period} to ${period+1} for Central Moving Average calculation\")\n",
    "                  return cmaCalc(period+1)\n",
    "              }\n",
    "              case _ => {\n",
    "                  return cmaCalc(period)\n",
    "              }\n",
    "          };\n",
    "      } catch {\n",
    "          case NonFatal(exc) => println(exc); return Array(0);\n",
    "      }\n",
    "  }\n",
    "    \n",
    "/** \n",
    "  Description (http://www.timestored.com/b/exponential-moving-average-ema-kdb/)\n",
    "  One of the issues with the simple moving average is that it gives every day an equal weighting. \n",
    "  For many purposes it makes more sense to give the more recent days a higher weighting, \n",
    "  one method of doing this is by using the Exponential Moving Average. \n",
    "  This uses an exponentially decreasing weight for dates further in the past.\n",
    "    \n",
    "  Method (http://www.itl.nist.gov/div898/handbook/pmc/section3/pmc324.htm)\n",
    "  params:\n",
    "      λ (lambda) - a constant that determines the depth of memory of the EWMA\n",
    "          The parameter λ determines the rate at which \"older\" data enter \n",
    "          into the calculation of the EWMA statistic. A value of λ=1 implies \n",
    "          that only the most recent measurement influences the EWMA (degrades to Shewhart chart). \n",
    "          Thus, a large value of λ (closer to 1) gives more weight to recent data \n",
    "          and less weight to older data; a small value of λ (closer to 0) gives \n",
    "          more weight to older data. The value of λ is usually set between 0.2 \n",
    "          and 0.3 (Hunter) although this choice is somewhat arbitrary. \n",
    "          Lucas and Saccucci (1990) give tables that help the user select λ.\n",
    "          \n",
    "      precision - number of significant digits in the calculated value\n",
    " */\n",
    "\n",
    "  def exponentialMA(lambda: Double = 0.5, precision: Int = 4): Array[Double] = {\n",
    "      val inverseLambda = (1.0 - lambda)\n",
    "      val ema = Array.fill(values.length)(0.0)\n",
    "      (values).view.toArray.zipWithIndex.map {\n",
    "          case (x,i) if i == 0 => {\n",
    "              ema(i) = roundME(x, precision)\n",
    "              ema(i)\n",
    "          }\n",
    "          case (x,i) => {\n",
    "              ema(i) = roundME((lambda * x) + (inverseLambda * ema(i-1)), precision)\n",
    "              ema(i)\n",
    "          }\n",
    "      }\n",
    "  }\n",
    "    \n",
    "  def sum(): Double = {\n",
    "      var sum = 0.0\n",
    "      var i = 0\n",
    "      while (i < values.length) { \n",
    "          sum += values(i); \n",
    "          i += 1 \n",
    "      }\n",
    "      sum\n",
    "  }\n",
    "    \n",
    "  def toArray(): Array[Double] = {\n",
    "      return values\n",
    "  }\n",
    "    \n",
    "  def length: Int = {\n",
    "      return values.length\n",
    "  }\n",
    "      \n",
    "  override def toString: String = {\n",
    "      \"Timeseries Object derived from a Spark SQL Dataframe for compatibility with a Lightning Visualization Server\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dftest1 = createTestDF(Array(2.0,2.0,4.0,4.0,6.0,6.0,8.0,8.0,10.0,10.0))\n",
    "metaPrint(dftest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val dftest2 = createTestDF(Array(1.0,2.0,3.0,4.0,8.0,10.0,20.0))\n",
    "metaPrint(dftest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanite data probes launched ... \n",
      "Nanite data probes launched ... \n"
     ]
    }
   ],
   "source": [
    "val testclass1 = new notebookTimeSeries(dftest1, Seq(\"TestValues\"))\n",
    "val testclass2 = new notebookTimeSeries(dftest2, Seq(\"TestValues\"))\n",
    "testclass1.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: window size of 0 provided. Central Moving Average cannot be computed\n",
      "WARNING: window size of 1 provided. Central Moving Average cannot be computed\n",
      "WARNING: Even window size of 2 detected. Incrementing window size from 2 to 3 for Central Moving Average calculation\n",
      "WARNING: Even window size of 4 detected. Incrementing window size from 4 to 5 for Central Moving Average calculation\n"
     ]
    }
   ],
   "source": [
    "val test1:Array[Double] = testclass1.simpleMA(2)\n",
    "val test2:Array[Double] = testclass1.simpleMA(4)\n",
    "val test3:Array[Double] = testclass1.centralMA(0)\n",
    "val test4:Array[Double] = testclass1.centralMA(1)\n",
    "val test5:Array[Double] = testclass1.centralMA(2)\n",
    "val test6:Array[Double] = testclass1.centralMA(3)\n",
    "val test7:Array[Double] = testclass1.centralMA(4)\n",
    "val test8:Array[Double] = testclass1.centralMA(5)\n",
    "val test9:Array[Double] = testclass2.exponentialMA(0.7,8)\n",
    "val test10:Array[Double] = testclass1.firstdiffy()\n",
    "val test11:Array[Double] = testclass1.secdiffy()\n",
    "val test12:Array[Double] = testclass1.toArray()\n",
    "val test13:Double = testclass1.sum()\n",
    "val test14:Int = testclass1.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving Average Test Suite ...\n",
      "=============================\n",
      "\n",
      "Test 1: Simple Moving Average for Window Size of 2\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 3.0 Calculated: 3.0   PASS\n",
      "Expected: 4.0 Calculated: 4.0   PASS\n",
      "Expected: 5.0 Calculated: 5.0   PASS\n",
      "Expected: 6.0 Calculated: 6.0   PASS\n",
      "Expected: 7.0 Calculated: 7.0   PASS\n",
      "Expected: 8.0 Calculated: 8.0   PASS\n",
      "Expected: 9.0 Calculated: 9.0   PASS\n",
      "\n",
      "Test 2: Simple Moving Average for Window Size of 4\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 3.0 Calculated: 3.0   PASS\n",
      "Expected: 4.0 Calculated: 4.0   PASS\n",
      "Expected: 5.0 Calculated: 5.0   PASS\n",
      "Expected: 6.0 Calculated: 6.0   PASS\n",
      "Expected: 7.0 Calculated: 7.0   PASS\n",
      "Expected: 8.0 Calculated: 8.0   PASS\n",
      "Expected: 9.0 Calculated: 9.0   PASS\n",
      "\n",
      "Test 3: Central Moving Average for Window Size of 0\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "\n",
      "Test 4: Central Moving Average for Window Size of 1\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "\n",
      "Test 5: Central Moving Average for Window Size of 2\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.667 Calculated: 2.667   PASS\n",
      "Expected: 3.333 Calculated: 3.333   PASS\n",
      "Expected: 4.667 Calculated: 4.667   PASS\n",
      "Expected: 5.333 Calculated: 5.333   PASS\n",
      "Expected: 6.667 Calculated: 6.667   PASS\n",
      "Expected: 7.333 Calculated: 7.333   PASS\n",
      "Expected: 8.667 Calculated: 8.667   PASS\n",
      "Expected: 9.333 Calculated: 9.333   PASS\n",
      "\n",
      "Test 6: Central Moving Average for Window Size of 3\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.667 Calculated: 2.667   PASS\n",
      "Expected: 3.333 Calculated: 3.333   PASS\n",
      "Expected: 4.667 Calculated: 4.667   PASS\n",
      "Expected: 5.333 Calculated: 5.333   PASS\n",
      "Expected: 6.667 Calculated: 6.667   PASS\n",
      "Expected: 7.333 Calculated: 7.333   PASS\n",
      "Expected: 8.667 Calculated: 8.667   PASS\n",
      "Expected: 9.333 Calculated: 9.333   PASS\n",
      "\n",
      "Test 7: Central Moving Average for Window Size of 4\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 3.6 Calculated: 3.6   PASS\n",
      "Expected: 4.4 Calculated: 4.4   PASS\n",
      "Expected: 5.6 Calculated: 5.6   PASS\n",
      "Expected: 6.4 Calculated: 6.4   PASS\n",
      "Expected: 7.6 Calculated: 7.6   PASS\n",
      "Expected: 8.4 Calculated: 8.4   PASS\n",
      "\n",
      "Test 8: Central Moving Average for Window Size of 5\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 3.6 Calculated: 3.6   PASS\n",
      "Expected: 4.4 Calculated: 4.4   PASS\n",
      "Expected: 5.6 Calculated: 5.6   PASS\n",
      "Expected: 6.4 Calculated: 6.4   PASS\n",
      "Expected: 7.6 Calculated: 7.6   PASS\n",
      "Expected: 8.4 Calculated: 8.4   PASS\n",
      "\n",
      "Test 9: Exponential Moving Average for Smoothing Factor of 0.7\n",
      "Expected: 1.0 Calculated: 1.0   PASS\n",
      "Expected: 1.7 Calculated: 1.7   PASS\n",
      "Expected: 2.61 Calculated: 2.61   PASS\n",
      "Expected: 3.583 Calculated: 3.583   PASS\n",
      "Expected: 6.6749 Calculated: 6.6749   PASS\n",
      "Expected: 9.00247 Calculated: 9.00247   PASS\n",
      "Expected: 16.700741 Calculated: 16.700741   PASS\n",
      "\n",
      "Test 10: First Differential (Slopes between each point)\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 0.0 Calculated: 0.0   PASS\n",
      "\n",
      "Test 11: Second Differential (Rate of change of slopes)\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "\n",
      "Test 12: Single Array of the original input dataframe columns summed\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 2.0 Calculated: 2.0   PASS\n",
      "Expected: 4.0 Calculated: 4.0   PASS\n",
      "Expected: 4.0 Calculated: 4.0   PASS\n",
      "Expected: 6.0 Calculated: 6.0   PASS\n",
      "Expected: 6.0 Calculated: 6.0   PASS\n",
      "Expected: 8.0 Calculated: 8.0   PASS\n",
      "Expected: 8.0 Calculated: 8.0   PASS\n",
      "Expected: 10.0 Calculated: 10.0   PASS\n",
      "Expected: 10.0 Calculated: 10.0   PASS\n",
      "\n",
      "Test 13: Sum of the single array\n",
      "Expected: 60.0 Calculated: 60.0   PASS\n",
      "\n",
      "Test 14: Length of the single array\n",
      "Expected: 10.0 Calculated: 10.0   PASS\n"
     ]
    }
   ],
   "source": [
    "val expected_1       = Array(0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0)\n",
    "val expected_2       = Array(0.0,0.0,0.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0)\n",
    "val expected_3_4     = Array(0.0)\n",
    "val expected_5_6     = Array(0.0,2.667,3.333,4.667,5.333,6.667,7.333,8.667,9.333)\n",
    "val expected_7_8     = Array(0.0,0.0,3.6,4.4,5.6,6.4,7.6,8.4)\n",
    "val expected_9       = Array(1.0,1.7,2.61,3.583,6.6749,9.00247,16.700741)\n",
    "val expected_10      = Array(0.0,2.0,0.0,2.0,0.0,2.0,0.0,2.0,0.0)\n",
    "val expected_11      = Array(2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0)\n",
    "val expected_12      = Array(2.0,2.0,4.0,4.0,6.0,6.0,8.0,8.0,10.0,10.0)\n",
    "val expected_13      = 60.0\n",
    "val expected_14      = 10\n",
    "var alpha = 0.7\n",
    "\n",
    "println(\"Moving Average Test Suite ...\")\n",
    "println(\"=============================\")\n",
    "var window = 2\n",
    "println(s\"\\nTest 1: Simple Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_1,test1)\n",
    "var window = 4\n",
    "println(s\"\\nTest 2: Simple Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_2,test2)\n",
    "var window = 0\n",
    "println(s\"\\nTest 3: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_3_4,test3)\n",
    "var window = 1\n",
    "println(s\"\\nTest 4: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_3_4,test4)\n",
    "var window = 2\n",
    "println(s\"\\nTest 5: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_5_6,test5)\n",
    "var window = 3\n",
    "println(s\"\\nTest 6: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_5_6,test6)\n",
    "var window = 4\n",
    "println(s\"\\nTest 7: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_7_8,test7)\n",
    "var window = 5\n",
    "println(s\"\\nTest 8: Central Moving Average for Window Size of $window\")\n",
    "printAnalysis(expected_7_8,test8)\n",
    "println(s\"\\nTest 9: Exponential Moving Average for Smoothing Factor of $alpha\")\n",
    "printAnalysis(expected_9,test9)\n",
    "println(s\"\\nTest 10: First Differential (Slopes between each point)\")\n",
    "printAnalysis(expected_10,test10)\n",
    "println(s\"\\nTest 11: Second Differential (Rate of change of slopes)\")\n",
    "printAnalysis(expected_11,test11)\n",
    "println(s\"\\nTest 12: Single Array of the original input dataframe columns summed\")\n",
    "printAnalysis(expected_12,test12)\n",
    "println(s\"\\nTest 13: Sum of the single array\")\n",
    "printAnalysis(expected_13,test13)\n",
    "println(s\"\\nTest 14: Length of the single array\")\n",
    "printAnalysis(expected_14,test14)\n",
    "println(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"/><meta http-equiv=\"X-UA-Compatible\" content=\"IE=10; IE=9; IE=8; IE=7; IE=EDGE\"/><meta charset=\"UTF-8\"/><link href=\"https://fonts.googleapis.com/css?family=Open+Sans:400,700\" rel=\"stylesheet\" type=\"text/css\"/><link rel=\"stylesheet\" href=\"https://lightningviz.mybluemix.net/css/app.css\"/><div id=\"lightning-body\" class=\"container content wrap push\"><div class=\"feed-container\"><div data-model=\"visualization\" data-model-id=\"7a65a81f-1e2e-49bc-a0a7-44f66aa38940\" class=\"feed-item-container\"><div data-type=\"lightning-line\" data-data=\"{&quot;series&quot;:[[2,2,4,4,6,6,8,8,10,10]],&quot;xaxis&quot;:&quot;x-Axis&quot;,&quot;yaxis&quot;:&quot;y-Axis&quot;}\" data-images=\"null\" data-options=\"{&quot;zoom&quot;:false}\" id=\"viz-7a65a81f-1e2e-49bc-a0a7-44f66aa38940\" data-initialized=\"false\" class=\"feed-item\"></div></div></div></div><script>window.lightning = window.lightning || {};\n",
       "window.lightning.host = \"https://lightningviz.mybluemix.net/\" || 'http://127.0.0.1:3000/';\n",
       "window.lightning.vizCount = (window.lightning.vizCount + 1) || 1;\n",
       "window.lightning.requiredVizTypes = window.lightning.requiredVizTypes || [];\n",
       "if(window.lightning.requiredVizTypes.indexOf(\"line\") === -1) {\n",
       "    window.lightning.requiredVizTypes.push(\"line\");\n",
       "}\n",
       "window._require = window.require;\n",
       "//window.require = undefined;\n",
       "window._define = window.define;\n",
       "window.define = undefined;</script><script src=\"https://lightningviz.mybluemix.net/js/embed.js\"></script>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testclass1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}